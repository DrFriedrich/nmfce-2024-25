###W10-W1_parta1_start_py
from scipy.optimize import minimize, rosen, rosen_der

x0 = [1.3, 0.7, 0.8, 1.9, 1.2]

# A simple application of the Nelder-Mead method is
res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)
print("Minimum of the Rosenbrock function with Nelder-Mead:", res.x)

# Now using the BFGS algorithm, using the first derivative and a few options
res = minimize(rosen, x0, method='BFGS', jac=rosen_der,
               options={'gtol': 1e-6, 'disp': True})

print("Minimum of the Rosenbrock function with BFGS:", res.x)

print("Solver return message:", res.message)
print("Hessian matrix =", res.hess_inv)
###W10-W1_parta1_end

###W10-W1_parta2_start_py
from scipy.optimize import minimize

# The objective function is
fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2
# Check the documentation of lambda functions if you haven't come across them yet

# There are three constraints defined as
cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},
        {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},
        {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})

# And variables must be positive, hence the following bounds
bnds = ((0, None), (0, None))

# The optimization problem is solved using the SLSQP method as
res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,
               constraints=cons)

print("Output of the constrained mimimisation:\n", res)
###W10-W1_parta2_end

###W10-W1_partb_start_py
def problem_b(x):
    """Problem formulation"""
    return -(126*x[0] - 9*x[0]**2 + 182*x[1] - 13*x[1]**2)

from scipy.optimize import minimize
x0 = [0, 0]

res = minimize(problem_b, x0, method='Nelder-Mead', tol=1e-6)
print(res)
print("Function value {}".format(-problem_b(res.x)))
###W10-W1_partb_end

###W10-W1_partc_start_md
The original solution is at $x=[7,7]$. Let's add non-negativity constraints as well as an inequality constraint that excludes this minimum. An example would be the following inequality constraint
$$
x_1 + x_2 \le 13
$$

###W10-W1_partc_switch_py
# The inequality constraints are defined as >= 0 so we need to move x_1 and x_2 to the right hand side
cons = ({'type': 'ineq', 'fun': lambda x:  13 - x[0] - x[1]})

# And variables must be positive, hence the following bounds
bnds = ((0, None), (0, None))

# The optimization problem is solved using the SLSQP method as
res = minimize(problem_b, (0, 0), method='SLSQP', bounds=bnds,
               constraints=cons)

print("Output of the constrained mimimisation:\n", res)
###W10-W1_partc_end

###W10-W1_partd_start_py
def problem_d(x):
    """Problem formulation"""
    return -(54*x[0] - 9*x[0]**2 + 78*x[1] - 13*x[1]**2)

from scipy.optimize import minimize
x0 = [0, 0]

res = minimize(problem_d, x0, method='Nelder-Mead', tol=1e-6)
print(res)
print("Function value {}".format(-problem_d(res.x)))
###W10-W1_partd_end

###W10-W1_parte_start_py
def problem_e(x):
    """Problem formulation"""
    return (x[0]**2 + x[0]*(1-x[1]) + x[1]**2 - x[1]*x[2] + x[2]**2 + x[2])
            
from scipy.optimize import minimize
x0 = [0, 0, 0]

res = minimize(problem_e, x0, method='Nelder-Mead', tol=1e-6)
print(res)
print("Function value {}".format(problem_e(res.x)))
###W10-W1_parte_end
